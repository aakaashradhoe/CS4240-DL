{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math as math\n",
    "import glob\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/nyoki-mtl/pytorch-segmentation/blob/master/src/models/scse.py\n",
    "#Import SCSE module\n",
    "class SCSEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel // reduction)),\n",
    "                                                nn.ReLU(inplace=True),\n",
    "                                                nn.Linear(int(channel // reduction), channel))\n",
    "        self.spatial_se = nn.Conv2d(channel, 1, kernel_size=1,\n",
    "                                    stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "\n",
    "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "        chn_se = self.avg_pool(x).view(bahs, chs)\n",
    "        chn_se = torch.sigmoid(self.channel_excitation(chn_se).view(bahs, chs, 1, 1))\n",
    "        chn_se = torch.mul(x, chn_se)\n",
    "        alpha =1\n",
    "        spa_se = torch.sigmoid(self.spatial_se(x))\n",
    "        spa_se = torch.mul(x, spa_se)\n",
    "        return torch.add(chn_se, spa_se)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dice Loss Function \n",
    "def dice_score(x, y):\n",
    "    a,b,c,d, = x.size()\n",
    "    d = torch.zeros(a)\n",
    "    for i in range(a):\n",
    "        lower = (x[i]+y[i]).sum()\n",
    "        upper = (x[i]*y[i]).sum()\n",
    "        d[i] = 1-2*(upper/lower)\n",
    "    loss = d.sum()/a\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Edited version of the Resnet34\n",
    "try:\n",
    "    from torch.hub import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        ##################\n",
    "        ####  Here we add the decoder layers ####\n",
    "        ##################\n",
    "        # Works only for resnet 34 (i think)\n",
    "        # Input parameters not 100% sure\n",
    "        self.conv1_1x1 = torch.nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(1,1), stride=1,padding=0)\n",
    "        self.conv2_1x1 = torch.nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(1,1), stride=1,padding=0)\n",
    "        self.conv3_1x1 = torch.nn.Conv2d(in_channels=128,out_channels=128,kernel_size=(1,1), stride=1,padding=0)\n",
    "        self.conv4_1x1 = torch.nn.Conv2d(in_channels=256,out_channels=128,kernel_size=(1,1), stride=1,padding=0)\n",
    "        self.conv5_1x1 = torch.nn.Conv2d(in_channels=512,out_channels=512,kernel_size=(1,1), stride=1,padding=0)\n",
    "        # use kaiming uniform or normal to initialize weights\n",
    "        self.Tconv4 = torch.nn.ConvTranspose2d(in_channels=512, out_channels=128, kernel_size=(2,2), stride=2, padding=0)\n",
    "        torch.nn.init.kaiming_normal_(self.Tconv4.weight)\n",
    "        self.Tconv3 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(2,2), stride=2, padding=0) \n",
    "        torch.nn.init.kaiming_normal_(self.Tconv3.weight)\n",
    "        self.Tconv2 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(2,2), stride=2, padding=0)\n",
    "        torch.nn.init.kaiming_normal_(self.Tconv2.weight)\n",
    "        self.Tconv1 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(2,2), stride=2, padding=0)\n",
    "        torch.nn.init.kaiming_normal_(self.Tconv1.weight)\n",
    "        self.SCSE1 = SCSEBlock(channel=256)\n",
    "        self.SCSE2 = SCSEBlock(channel=256)\n",
    "        self.SCSE3 = SCSEBlock(channel=256)\n",
    "        self.SCSE4 = SCSEBlock(channel=256)\n",
    "        self.bn_decoder1 = norm_layer(256)\n",
    "        self.bn_decoder2 = norm_layer(256)\n",
    "        self.bn_decoder3 = norm_layer(256)\n",
    "        self.bn_decoder4 = norm_layer(256)\n",
    "        self.Tconvfinal = torch.nn.ConvTranspose2d(in_channels=256, out_channels=1, kernel_size=(2,2), stride=2, padding=0)\n",
    "        torch.nn.init.kaiming_normal_(self.Tconvfinal.weight)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "        self.group1 = nn.Sequential(self.conv1,self.bn1,self.relu,self.maxpool,self.layer1,self.layer2)\n",
    "        self.group2 = nn.Sequential(self.layer3, self.layer4)\n",
    "        self.group3 = nn.Sequential(self.conv1_1x1,self.conv2_1x1,self.conv3_1x1,self.conv4_1x1,self.conv5_1x1,self.Tconv4,\n",
    "                               self.Tconv3,self.Tconv2,self.Tconv1,self.SCSE1,self.SCSE2,self.SCSE3,self.SCSE4,self.bn_decoder1,self.bn_decoder2,self.bn_decoder3,self.bn_decoder4,self.Tconvfinal,self.sigmoid)\n",
    "        \n",
    "        #\n",
    "        #\n",
    "        #These are not needed now\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        # Resnet Encoder\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        self.cache1 = torch.clone(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        self.cache2 = torch.clone(x)\n",
    "        x = self.layer2(x)\n",
    "        self.cache3 = torch.clone(x)\n",
    "        x = self.layer3(x)\n",
    "        self.cache4 = torch.clone(x)\n",
    "        x = self.layer4(x)\n",
    "        # Decoder here\n",
    "        x = self.conv5_1x1(x)\n",
    "        x = self.Tconv4(x)\n",
    "        self.cache4 = self.conv4_1x1(self.cache4)\n",
    "        x = torch.cat((self.cache4, x),1)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_decoder4(x)\n",
    "        x = self.SCSE4(x)\n",
    "        \n",
    "        x = self.Tconv3(x)\n",
    "        self.cache3 = self.conv3_1x1(self.cache3)\n",
    "        x = torch.cat((self.cache3, x),1)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_decoder3(x)\n",
    "        x = self.SCSE3(x)\n",
    "\n",
    "        x = self.Tconv2(x)\n",
    "        self.cache2 = self.conv2_1x1(self.cache2)\n",
    "        x = torch.cat((self.cache2, x),1)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_decoder2(x)\n",
    "        x = self.SCSE2(x)\n",
    "\n",
    "        x = self.Tconv1(x)\n",
    "        self.cache1 = self.conv1_1x1(self.cache1)\n",
    "        x = torch.cat((self.cache1, x),1)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_decoder1(x)\n",
    "        x = self.SCSE1(x)\n",
    "        x = self.Tconvfinal(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### This sections is for importing the data and validatio sets from local device to python.\n",
    "# the data is normalized too.####\n",
    "\n",
    "def list_images(image_dir, filename_expression='*.jpg'):\n",
    "        filenames = glob.glob(os.path.join(image_dir, filename_expression))\n",
    "        filenames = sorted(filenames) # important for cross-platform compatiblity\n",
    "        print(f'Found {len(filenames)} image files in the directory \"{image_dir}\"')\n",
    "        return filenames\n",
    "def list_images2(image_dir, filename_expression='*.PNG'):\n",
    "        filenames = glob.glob(os.path.join(image_dir, filename_expression))\n",
    "        filenames = sorted(filenames) # important for cross-platform compatiblity\n",
    "        print(f'Found {len(filenames)} image files in the directory \"{image_dir}\"')\n",
    "        return filenames\n",
    "    \n",
    "def import_and_format_data(dataset,type_data):\n",
    "\n",
    "    #reference image (all images should have same tensor size as this one)\n",
    "    N = len(dataset)\n",
    "    # define placeholder for tensor data\n",
    "    if type_data == 'dataset':\n",
    "        a,b,c = (transforms.ToTensor()(plt.imread(dataset[0]))).size()\n",
    "        dataset2 = torch.zeros(N,a,b,c)\n",
    "    if type_data == 'validationset':\n",
    "        a,b,c = (transforms.ToTensor()(plt.imread(dataset[0]))).size()\n",
    "        dataset2 = torch.zeros(N,b,c)\n",
    "    \n",
    "    \n",
    "    #define forloop for importing data to tensors taking into account unwanted rotations and sizes\n",
    "    for i in range(N):\n",
    "        # read tensors to be able to check sizes\n",
    "        T_data = transforms.ToTensor()(plt.imread(dataset[i])) \n",
    "        T_val =  transforms.ToTensor()(plt.imread(dataset[i])) \n",
    "        #perform checks to see if resizing or rotating is needed\n",
    "        if b == T_data.size(1) and c==T_data.size(1):\n",
    "            dataset2[i] = transforms.ToTensor()(plt.imread(dataset[i]))\n",
    "        elif c == T_data.size(1) and b==T_data.size(2):\n",
    "            if type_data == 'dataset':\n",
    "                dataset2[i] = transforms.ToTensor()((transforms.ToPILImage()(T_data).convert(\"RGB\")).rotate(90, expand=True))\n",
    "            elif type_data == 'validationset':\n",
    "                dataset2[i] = transforms.ToTensor()((transforms.ToPILImage()(T_data)).rotate(90, expand=True))       \n",
    "        else: \n",
    "            if round(b/c)!=round((T_data.size(1))/(T_data.size(1))):\n",
    "                print('ratio height width unequal','reference=(',b,',',c,') and target=()',T_data.size(1),',',T_data.size(1),')')\n",
    "            dataset2[i]= TF.resize(T_data,[b,c]) \n",
    "    return dataset2\n",
    "    \n",
    "\n",
    "    \n",
    "    # Select dataset and directory of dataset (unpack the cfd_image and seg_gt in CFD and testcrop and traincrop in CRACK500)\n",
    "dataset_for_model = 'CFD'\n",
    "Selected_data_directory = 'Data1\\\\CFD\\\\'\n",
    "\n",
    "\n",
    "if dataset_for_model =='CFD':\n",
    "\n",
    "    ##import data from local device. !!You need to set this to directory of your data!! ##\n",
    "    IMAGE_DIR = Selected_data_directory + '\\\\cfd_image'\n",
    "    IMAGE_DIR2 = Selected_data_directory + '\\\\seg_gt'\n",
    "\n",
    "    ## This part turns the dataset and validationset in usable torch tensors in desired format (Number of images, channels, width, height) ##\n",
    "    N_train = 72\n",
    "    N_test = 46\n",
    "    #put data in list and read the length N of the list\n",
    "    dataset = list_images(IMAGE_DIR)\n",
    "    validationset = list_images2(IMAGE_DIR2)\n",
    "    N = len(dataset)\n",
    "    if N!=len(validationset):\n",
    "        print('dataset and validationset do not match in size. Dataset=',N,'and validationset=',len(validationset))\n",
    "    dataset = import_and_format_data(dataset,'dataset')\n",
    "    validationset = import_and_format_data(validationset,'validationset')\n",
    "\n",
    "if dataset_for_model == \"CRACK500\":\n",
    "\n",
    "    ##import data from local device. \n",
    "    IMAGE_DIR = Selected_data_directory+'\\\\traincrop' \n",
    "    IMAGE_DIR2 = Selected_data_directory+'\\\\testcrop'\n",
    "\n",
    "    ## This part turns the dataset and validationset in usable torch tensors in desired format (Number of images, channels, width, height) ##\n",
    "    N_train = 3792\n",
    "    N_test = 2248\n",
    "    #put data in list and read the length N of the list\n",
    "    train_dataset = list_images(IMAGE_DIR)\n",
    "    train_validationset = list_images2(IMAGE_DIR)\n",
    "    test_dataset = list_images(IMAGE_DIR2)\n",
    "    test_validationset = list_images2(IMAGE_DIR2)\n",
    "    \n",
    "    dataset = train_dataset + test_dataset # dataset is actually total 3020 mistake in paper so trainset =1812 and testset =1208\n",
    "    validationset = train_validationset + test_validationset \n",
    "    \n",
    "    N = len(dataset)\n",
    "    if N!=len(validationset):\n",
    "        print('dataset and validationset do not match in size. Dataset=',N,'and validationset=',len(validationset))\n",
    "        \n",
    "    dataset = import_and_format_data(dataset,'dataset')\n",
    "    validationset = import_and_format_data(validationset,'validationset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A data sorter which randomly turns the dataset into a train dataset and a test dataset and accordingly \n",
    "# the train validationset and test validationset. The ratio images in the train test is approx 3:2 ####\n",
    "\n",
    "def alt_Datasorter(dataset, validationset, N, N_train, N_test):\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    a,b,c,d = dataset.size()\n",
    "    e,g,h = validationset.size()\n",
    "    f = 1\n",
    "    train_dataset = torch.zeros(N_train,b,c,d)\n",
    "    test_dataset = torch.zeros(N_test,b,c,d)\n",
    "    train_validationset = torch.zeros(N_train,f,g,h)\n",
    "    test_validationset = torch.zeros(N_test,f,g,h)\n",
    "    \n",
    "    for i in range(N):\n",
    "        k = np.random.rand(1)\n",
    "        if k < 0.5:\n",
    "            if count_train < N_train:\n",
    "                train_dataset[count_train,:] = dataset[i,:]\n",
    "                train_validationset[count_train] = validationset[i]\n",
    "                count_train += 1\n",
    "            else:\n",
    "                test_dataset[count_test] = dataset[i]\n",
    "                test_validationset[count_test] = validationset[i]\n",
    "                count_test += 1\n",
    "        else:\n",
    "            if count_test < N_test:\n",
    "                test_dataset[count_test] = dataset[i]\n",
    "                test_validationset[count_test] = validationset[i]\n",
    "                count_test += 1\n",
    "            else:\n",
    "                train_dataset[count_train] = dataset[i]\n",
    "                train_validationset[count_train] = validationset[i]\n",
    "                count_train += 1\n",
    "    return train_dataset, test_dataset, train_validationset, test_validationset\n",
    "\n",
    "train_dataset, test_dataset, train_validationset, test_validationset = alt_Datasorter(dataset, validationset, N, N_train, N_test)\n",
    "\n",
    "train_input = [train_dataset, train_validationset]\n",
    "test_input = [test_dataset, test_validationset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### function which preprocesses batches of the data- and validationsets. For each image within the batch random rotations\n",
    "#, flips and brightness changes are applied ####\n",
    "def Transforms(x_batch, y_batch, d):\n",
    "    for i in range(len(x_batch)):\n",
    "        # randomly rotate the data set and validation set in a single angle \n",
    "        k = random.random()*360\n",
    "        x_batch[i] = TF.rotate(x_batch[i], k)\n",
    "        y_batch[i] = TF.rotate(y_batch[i], k)\n",
    "        # randomly flip the data set and validation set in the horizontal and vertical direction\n",
    "        if random.random() > 0.5:\n",
    "            x_batch[i] = TF.hflip(x_batch[i])\n",
    "            y_batch[i] = TF.hflip(y_batch[i])\n",
    "        if random.random() > 0.5:\n",
    "            x_batch[i] = TF.vflip(x_batch[i])\n",
    "            y_batch[i] = TF.vflip(y_batch[i]) \n",
    "        # randomly adjust the brightness of the images with a change of 0.05\n",
    "        if random.random() > 0.5:\n",
    "            x_batch[i] = TF.adjust_hue(x_batch[i],0.05)\n",
    "        else:\n",
    "            x_batch[i] = TF.adjust_hue(x_batch[i],-0.05)\n",
    "        # randomly adjust the contrast of the images with a change of 0.05\n",
    "        if random.random() > 0.5:\n",
    "            x_batch[i] = TF.adjust_contrast(x_batch[i],1.05)\n",
    "        else:\n",
    "            x_batch[i] = TF.adjust_contrast(x_batch[i],0.95)\n",
    "        y_batch[i] = torch.round(y_batch[i])\n",
    "    x_batch = TF.center_crop(x_batch, [320,320])\n",
    "    y_batch = TF.center_crop(y_batch, [320,320])\n",
    "    x_batch = TF.resize(x_batch,d)\n",
    "    y_batch = TF.resize(y_batch,d)\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All algorithm for calculation the precision, recall and F1 value for the predicted value and the True value. ##\n",
    "def precision_recall_f1(test_dataset, test_validationset):\n",
    "    N_batches = len(test_dataset)\n",
    "    biggest_batch_a, biggest_batch_b,biggest_batch_c,biggest_batch_d = test_dataset[0].shape\n",
    "    smallest_batch_a, smallest_batch_b, smallest_batch_c, smallest_batch_d = test_dataset[-1].shape\n",
    "    N = (N_batches-1)*biggest_batch_a + smallest_batch_a\n",
    "    Pr = torch.zeros(N)\n",
    "    Re = torch.zeros(N)\n",
    "    F1 = torch.zeros(N)\n",
    "    for k in range(N_batches):\n",
    "        with torch.no_grad():\n",
    "            x_batch = test_dataset[k]\n",
    "            y_batch = test_validationset[k]\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = dice_score(y_pred, y_batch)\n",
    "            \n",
    "            a,b,c,d = y_batch.shape\n",
    "            y_k = y_pred\n",
    "            y_k2 = y_batch\n",
    "            y_predict = y_pred.reshape(a,c*d)\n",
    "            y_batches = y_batch.reshape(a,c*d)\n",
    "    \n",
    "            for i in range(a):\n",
    "                n = (k*biggest_batch_a)+i\n",
    "                TP,FP,FN,TN,NOT_SORTED = 0,0,0,0,0\n",
    "                y_predict[i] = y_predict[i]-torch.min(y_predict[i])\n",
    "                y_predict[i] = y_predict[i]/(torch.max(y_predict[i]))\n",
    "                y_predict[i] = torch.round(y_predict[i])\n",
    "                y_batches[i] = y_batches[i]-torch.min(y_batches[i])\n",
    "                y_batches[i] = y_batches[i]/(torch.max(y_batches[i]))\n",
    "                y_batches[i] = torch.round(y_batches[i])\n",
    "\n",
    "                for j in range(c*d):\n",
    "                    shape = 'diamond'\n",
    "                    # True positives are pixels that are either diamond or square shape in distance 2 pixel of ground truth\n",
    "                    if j>d*2 and j<c*d-2*d:\n",
    "                        if y_predict[i,j]==1:\n",
    "                            if  y_batches[i,j]==1:\n",
    "                                TP +=1\n",
    "                            elif j%d!=0 and y_batches[i,j-1]==1 :\n",
    "                                TP +=1\n",
    "                            elif j%d!=d-1 and y_batches[i,j+1]==1 :\n",
    "                                TP +=1\n",
    "                            elif j%d!=1 and y_batches[i,j-2]==1 :\n",
    "                                TP +=1\n",
    "                            elif j%d!=d-2 and y_batches[i,j+2]==1:\n",
    "                                TP +=1\n",
    "                            elif j-d>=0 and y_batches[i,j-d]==1:\n",
    "                                TP +=1\n",
    "                            elif j-d>=0 and j%d!=0 and y_batches[i,j-d-1]==1:\n",
    "                                TP +=1\n",
    "                            elif j-d>=0 and j%d!=d-1 and y_batches[i,j-d+1]==1:\n",
    "                                TP +=1\n",
    "                            elif j-d>=0 and j%d!=1 and shape=='square' and y_batches[i,j-d-2]==1:\n",
    "                                TP +=1\n",
    "                            elif j-d>=0 and j%d!=d-2 and shape=='square' and y_batches[i,j-d+2]==1:\n",
    "                                TP +=1\n",
    "                            elif j-2*d>=0 and y_batches[i,j-2*d]==1:\n",
    "                                TP +=1\n",
    "                            elif j-2*d>=0 and j%d!=0 and shape=='square' and y_batches[i,j-2*d-1]==1:\n",
    "                                TP +=1\n",
    "                            elif j-2*d>=0 and j%d!=d-1 and shape=='square' and y_batches[i,j-2*d+1]==1:\n",
    "                                TP +=1\n",
    "                            elif j-2*d>=0 and j%d!=1 and shape=='square' and y_batches[i,j-2*d-2]==1:\n",
    "                                TP +=1\n",
    "                            elif j-2*d>=0 and j%d!=d-2 and shape=='square' and y_batches[i,j-2*d+2]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=d and y_batches[i,j+d]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=d and j%d!=0 and y_batches[i,j+d-1]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=d and j%d!=d-1 and y_batches[i,j+d+1]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=d and j%d!=1 and shape=='square' and y_batches[i,j+d-2]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=d and j%d!=d-2 and shape=='square' and y_batches[i,j+d+2]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=2*d and y_batches[i,j+d]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=2*d and j%d!=0 and shape=='square' and y_batches[i,j+2*d-1]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=2*d and j%d!=d-1 and shape=='square' and y_batches[i,j+2*d+1]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=2*d and j%d!=1 and shape=='square' and y_batches[i,j+2*d-2]==1:\n",
    "                                TP +=1\n",
    "                            elif c*d-j>=2*d and j%d!=d-2 and shape=='square' and y_batches[i,j+2*d+2]==1:\n",
    "                                TP +=1\n",
    "                            else:\n",
    "                                FP +=1\n",
    "                        elif y_predict[i,j]==0 and y_batches[i,j]==1:\n",
    "                            FN += 1\n",
    "                        elif y_predict[i,j]==0 and y_batches[i,j]==0:\n",
    "                            TN += 1\n",
    "                    else:\n",
    "                        if y_predict[i,j]==1 and y_batches[i,j]==1:\n",
    "                            TP += 1\n",
    "                        elif y_predict[i,j]==1 and  y_batches[i,j]==0:\n",
    "                            FP += 1\n",
    "                        elif y_predict[i,j]==0 and y_batches[i,j]==1:\n",
    "                            FN += 1\n",
    "                        elif y_predict[i,j]==0 and y_batches[i,j]==0:\n",
    "                            TN += 1\n",
    "                if TP+FP+FN+TN != c*d:\n",
    "                    print('Not all values are being evaluated check code',\n",
    "                          'fraction of data not sorted is',NOT_SORTED/(c*d))\n",
    "                if TP + FN > 0:\n",
    "                    Pr[n] = TP/(TP+FP)\n",
    "                if TP + FN > 0:\n",
    "                    Re[n] = TP/(TP+FN)\n",
    "                if  Pr[n] + Re[n] > 0:\n",
    "                    F1[n] = (2*Pr[n]*Re[n])/(Pr[n]+Re[n])\n",
    "                \n",
    "                print('Precision of train set:', Pr[n])\n",
    "                print('Recall of train set:', Re[n])\n",
    "                print('F1 of train set:', F1[n])\n",
    "                print('')\n",
    "  \n",
    "    Pr_final = torch.sum(Pr)/N\n",
    "    Re_final = torch.sum(Re)/N\n",
    "    F1_final = torch.sum(F1)/N\n",
    "    \n",
    "                \n",
    "    return Pr_final, Re_final, F1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternate dataloader for creating batches for the training of the neural network ##\n",
    "def alternate_dataloader(data, batchsize):\n",
    "    data , validation = [data[0],data[1]]\n",
    "    a,b,c,d = data.shape\n",
    "    e,f,g,h = validation.shape\n",
    "    batchesdata = torch.zeros(math.floor(a/batchsize),batchsize,b,c,d)\n",
    "    batchesvalidation= torch.zeros(math.floor(a/batchsize),batchsize,f,g,h)\n",
    "    batchesL = torch.zeros(a%batchsize,b,c,d)\n",
    "    batchesLvalidation = torch.zeros(a%batchsize,f,g,h)\n",
    "    LISTdata = []\n",
    "    LISTvalidation = []\n",
    "    LISTfinal = []\n",
    "    for i in range(math.ceil(a/batchsize)):\n",
    "        if a-i*batchsize > batchsize:\n",
    "            for j in range(batchsize):\n",
    "                d= data[j+i*batchsize]\n",
    "                batchesdata[i,j]= d\n",
    "                batchesvalidation[i,j]=validation[j+i*batchsize]\n",
    "\n",
    "        else:\n",
    "            for j in range(a%batchsize):\n",
    "                batchesL[j]=data[j+i*batchsize]\n",
    "                batchesLvalidation[j]=validation[j+i*batchsize]\n",
    "    \n",
    "    for i in range(math.floor(a/batchsize)):\n",
    "        LISTdata.append(batchesdata[i])\n",
    "        LISTvalidation.append(batchesvalidation[i])\n",
    "    LISTdata.append(batchesL)\n",
    "    LISTvalidation.append(batchesLvalidation)\n",
    "    LISTfinal = [LISTdata, LISTvalidation]\n",
    "    return LISTfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing pre-proccesed randomly batch data for training and testing\n",
    "train_loader = alternate_dataloader(train_input, batchsize=5)\n",
    "test_loader = alternate_dataloader(test_input, batchsize=5)\n",
    "\n",
    "train_dataset = train_loader[0]\n",
    "train_validationset = train_loader[1]\n",
    "test_dataset = test_loader[0]\n",
    "test_validationset = test_loader[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for training loop (non hyper-parameters)\n",
    "epochs = 45\n",
    "#set progressing image sizes\n",
    "size = [128,256,320] \n",
    "i = 0\n",
    "# Define list to store losses and performances of each iteration\n",
    "Pr = np.zeros(epochs)\n",
    "Re = np.zeros(epochs)\n",
    "F1 = np.zeros(epochs)\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare hyper-parameters for training\n",
    "#Set default learning rates\n",
    "learning_rate = 0.005\n",
    "learning_rate_group1 = learning_rate*(1/9)\n",
    "learning_rate_group2 = learning_rate*(1/3)\n",
    "learning_rate_group3 = learning_rate*1\n",
    "Beta_group1 = [0.9, 0.999]\n",
    "Beta_group2 = [0.9, 0.999]\n",
    "Beta_group3 =  [0.9, 0.999]\n",
    "eps_group1 = 10**(-8)\n",
    "eps_group2 = 10**(-8)\n",
    "eps_group3 = 10**(-8)\n",
    "# Set schedular and schedular parameters\n",
    "#Inputs for schedular\n",
    "base_lr= [0.000001*learning_rate_group1, \n",
    "          0.000001*learning_rate_group2, \n",
    "          0.000001*learning_rate_group3]\n",
    "max_lr=[learning_rate_group1, \n",
    "          learning_rate_group2, \n",
    "          learning_rate_group3]\n",
    "Total_number_of_batches_in_train=len(train_dataset)\n",
    "Total_number_of_steps = epochs*Total_number_of_batches_in_train\n",
    "steps_till_Turn_point_lr_max=np.ceil(Total_number_of_steps*0.4)\n",
    "steps_till_End = epochs*Total_number_of_batches_in_train - steps_till_Turn_point_lr_max\n",
    "\n",
    "# Custom Schedular for the Lr cycling\n",
    "scaling = 0.0001\n",
    "def customlinspace(delta_x,y1,y2, scaling):\n",
    "    y1 = y1\n",
    "    y2 = y2*scaling\n",
    "    delta_y = y2-y1\n",
    "    delta_x = delta_x\n",
    "    a = (delta_y / delta_x)\n",
    "    b = y1\n",
    "    x= np.linspace(0,delta_x,delta_x+1)\n",
    "    y= (a*x)+b\n",
    "    return y\n",
    "\n",
    "space1 = customlinspace(int(steps_till_Turn_point_lr_max),0.05, 1, 1)\n",
    "space2 = customlinspace(int(steps_till_End+1),1, 0.05, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main file for training and testing algorithm\n",
    "print('starting...')\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "def try_gpu():\n",
    "    \"\"\"\n",
    "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
    "    as cpu.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.group1.parameters(),learning_rate_group1)\n",
    "optimizer.add_param_group({'params':model.group2.parameters(),'lr':learning_rate_group2})\n",
    "optimizer.add_param_group({'params':model.group3.parameters(),'lr':learning_rate_group3})\n",
    "print(optimizer)\n",
    "print(learning_rate_group3)\n",
    "# Try using gpu instead of cpu\n",
    "device = try_gpu()\n",
    "\n",
    "#Intitialize first layer to be off\n",
    "for param in model.group1.parameters():\n",
    "    param.requires_grad = False\n",
    "T1=0\n",
    "T2=1\n",
    "lrs = []\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {:.0f}'.format(epoch+1))\n",
    "    #Unfreeze the first layer after 15 epocht\n",
    "    if epoch == 15:\n",
    "        for param in model.group1.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    # Network in training mode and to device\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "       \n",
    "    if epoch == 15:\n",
    "        i = 1\n",
    "    if epoch == 30:\n",
    "        i = 2\n",
    "    # Training loop \n",
    "    for j in range(len(train_dataset)):\n",
    "        # Learning rate cycler:\n",
    "        if int(epoch*Total_number_of_batches_in_train + j) < int(steps_till_Turn_point_lr_max):\n",
    "            optimizer.param_groups[0]['lr'] = space1[T1]*learning_rate_group1\n",
    "            optimizer.param_groups[1]['lr'] = space1[T1]*learning_rate_group2\n",
    "            optimizer.param_groups[2]['lr'] = space1[T1]*learning_rate_group3\n",
    "            T1=T1+1\n",
    "        else:\n",
    "            optimizer.param_groups[0]['lr'] = space2[T2]*learning_rate_group1\n",
    "            optimizer.param_groups[1]['lr'] = space2[T2]*learning_rate_group2\n",
    "            optimizer.param_groups[2]['lr'] = space2[T2]*learning_rate_group3\n",
    "            T2=T2+1\n",
    "            \n",
    "        x_batch = torch.clone(train_dataset[j])\n",
    "        y_batch = torch.clone(train_validationset[j])\n",
    "        x_batch, y_batch = Transforms(x_batch,y_batch,size[i])\n",
    "        \n",
    "        group3 = optimizer.param_groups[2]\n",
    "        lrs.append(group3[\"lr\"])\n",
    "        \n",
    "        # Set to same device\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        y_pred = model(x_batch)\n",
    "#         Compute the loss\n",
    "        loss =dice_score(y_pred, y_batch)\n",
    "        print(loss)\n",
    "        train_losses.append(loss)\n",
    "        # Backward computation and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    Pr2 = 0\n",
    "    Re2 = 0 \n",
    "    F12 = 0\n",
    "    \n",
    "    # test looop\n",
    "    if epoch == 44:\n",
    "        Pr_final, Re_final, F1_final = precision_recall_f1(test_dataset, test_validationset)\n",
    "        print('Precision of test set:', Pr_final)\n",
    "        print('Recall of test set:', Re_final)\n",
    "        print('F1 of test set:', F1_final)\n",
    "        print('')\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
